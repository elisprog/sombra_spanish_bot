{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13d7470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† –†–∞—Å–ø–æ–∑–Ω–∞—ë–º —Ç–µ–∫—Å—Ç (whisper)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ! –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ /Users/elizaveta/Documents/–ü—Ä–æ–µ–∫—Ç_2/7septimo.txt\n",
      "üîé –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤:\n",
      "\n",
      " Esta es la historia de Lola y Ana, quien comparten un piso en Barcelona, y de sus vecinos de en frente Pablo y Sam el americano. Lola quiere ser rica, Pablo quiere ser actor y Sam quiere invitar a Ana al cine. Prep√°rate para el pr√≥ximo episodio de Extra. No puedo creer que vuelvas de Argentina. ¬øRe\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "def transcribe_audio(file_path, model_size=\"medium\", language=\"es\"):\n",
    "    model = whisper.load_model(model_size)\n",
    "    result = model.transcribe(file_path, language=language)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def save_transcript(text, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_path = input(\"–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∞—É–¥–∏–æ—Ñ–∞–π–ª—É (.mp3, .wav –∏ –¥—Ä.): \").strip()\n",
    "\n",
    "    if not os.path.isfile(audio_path):\n",
    "        print(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"üß† –†–∞—Å–ø–æ–∑–Ω–∞—ë–º —Ç–µ–∫—Å—Ç (whisper)...\")\n",
    "    transcript = transcribe_audio(audio_path, model_size=\"medium\", language=\"es\")\n",
    "\n",
    "    output_file = audio_path.rsplit(\".\", 1)[0] + \".txt\"\n",
    "    save_transcript(transcript, output_file)\n",
    "\n",
    "    print(f\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ! –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}\")\n",
    "    print(\"üîé –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤:\\n\")\n",
    "    print(transcript[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd64635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF –ø–æ NOUN/VERB/ADJ/ADV):\n",
      "['horno', 'espa√±ol', 'quieres', 'llamo', 'perro', 'museo', 'americano', 'coches', 'chicas', 'vivo', 'correo', 'dormitorio', 'ducha', 'estupendo', 'est√°s', 'quedar', 'tienes', 'cama', 'carta', 'chica', 'chico', 'compras', 'dije', 'duerme', 'factura', 'gracias', 'hablar', 'queda', 'r√°pido', 'tomar', 'abajo', 'acab√≥', 'amigos', 'a√±os', 'ba√±o', 'bici', 'bicicleta', 'celebrarlo', 'corresponsal', 'dices', 'digas', 'dormir', 'echo', 'favor', 'febrero', 'fuertes', 'gusta', 'hablo', 'hombres', 'importa']\n",
      "\n",
      "–ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –±–∏–≥—Ä–∞–º–º—ã:\n",
      "['el horno', 'en el', 'el perro', 'me llamo', 'un museo', 'est√° en', 'de am√©rica', 'no no', 'en un', 'perro est√°', 'esto es', 'muy bien', 'es la', 'el correo', 'la factura', 'bien el', 'tomar algo', 'es un', 'quiere decir', 'con coches', 'es el', 'se vaya', 'un poco', 'perro de', 'duerme en', 'de compras', 'compras para', 'en espa√±a', 'le gusta', 'est√°n las', 'una carta', 'te lo', 'se acab√≥', 'y no', 'no me', 'lo siento', 'luis luis', 'que s√≠', 'correo luis', 'a ver', 'factura del', 'de la', 'oh es', 'es esto', 'para ti', 'te quiere', 'am√©rica am√©rica', 'hace siete', 'siete a√±os', 'espa√±ol bueno']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, bigrams\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def load_and_split_sentences(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()], text\n",
    "\n",
    "def extract_pos_filtered_words(text, allowed_pos={\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}):\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc \n",
    "            if token.pos_ in allowed_pos and not token.is_stop and token.is_alpha]\n",
    "\n",
    "def extract_keywords_by_pos(text, allowed_pos={\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PREP\"}, top_n=20):\n",
    "    words = extract_pos_filtered_words(text, allowed_pos)\n",
    "    filtered_text = \" \".join(words)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([filtered_text])\n",
    "    scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0])\n",
    "    sorted_keywords = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [word for word, _ in sorted_keywords[:top_n]]\n",
    "\n",
    "def extract_top_bigrams(text, top_n=20, exclude_words=None):\n",
    "    if exclude_words is None:\n",
    "        exclude_words = set()\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # –¢–æ–ª—å–∫–æ –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "\n",
    "    bigram_list = list(bigrams(tokens))\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–∏–≥—Ä–∞–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "    filtered_bigrams = [\n",
    "        pair for pair in bigram_list\n",
    "        if pair[0] not in exclude_words and pair[1] not in exclude_words\n",
    "    ]\n",
    "\n",
    "    bigram_freq = Counter(filtered_bigrams)\n",
    "    top_bigrams = [\" \".join(pair) for pair, _ in bigram_freq.most_common(top_n)]\n",
    "    return top_bigrams\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "sentences, text = load_and_split_sentences(\"1primero.txt\")\n",
    "\n",
    "# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ POS\n",
    "keywords = extract_keywords_by_pos(text, top_n=50)\n",
    "\n",
    "# –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –±–∏–≥—Ä–∞–º–º—ã\n",
    "exclude_names = {\"ana\", \"lola\", \"sam\", \"pablo\", \"antonio\"}\n",
    "bigrams = extract_top_bigrams(text, top_n=50, exclude_words=exclude_names)\n",
    "\n",
    "# –î–ª—è –≤—ã–≤–æ–¥–∞:\n",
    "print(\"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF –ø–æ NOUN/VERB/ADJ/ADV):\")\n",
    "print(keywords)\n",
    "\n",
    "print(\"\\n–ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –±–∏–≥—Ä–∞–º–º—ã:\")\n",
    "print(bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
